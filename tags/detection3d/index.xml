<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>detection3d on Scepter914 Website</title>
    <link>https://scepter914.github.io/tags/detection3d/</link>
    <description>Recent content in detection3d on Scepter914 Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2024 10:00:00 +0900</lastBuildDate><atom:link href="https://scepter914.github.io/tags/detection3d/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>RegTTA3D: Better Regression Makes Better Test-time Adaptive 3D Object Detection (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/001008_regtta3d/</link>
      <pubDate>Thu, 14 Nov 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/001008_regtta3d/</guid>
      <description>RegTTA3D: Better Regression Makes Better Test-time Adaptive 3D Object Detection (ECCV2024) Summary Test-time adaptationを用いた3D detection Domain adaptation Regressionを中心に少ないparameterでtuningできる Method Domain</description>
    </item>
    
    <item>
      <title>Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000972_detecting_as_labeling/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000972_detecting_as_labeling/</guid>
      <description>Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection (ECCV2024) Summary from Phigent Robotics CTOが Baidu -&amp;gt; Horizon Robotics の経歴 https://github.com/HuangJunJie2017/BEVDet Camera LiDAR 3D detection において、Camera pipelineはlabel推定にしか使わないようにした</description>
    </item>
    
    <item>
      <title>On-the-fly Category Discovery for LiDAR Semantic Segmentation (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/001003_ocdss/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/001003_ocdss/</guid>
      <description>On-the-fly Category Discovery for LiDAR Semantic Segmentation (ECCV2024) Summary Unknown objectのための LiDAR Semantic Segmentationに必要なカテゴライズの導入 Method Baseは &amp;ldquo;On-the-fly Category Discovery (CVPR 2023)&amp;rdquo; https://github.com/PRIS-CV/On-the-fly-Category-Discovery 解きたいタスクの違い (a)</description>
    </item>
    
    <item>
      <title>Towards Stable 3D Object Detection (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000969_towards_stable/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000969_towards_stable/</guid>
      <description>Towards Stable 3D Object Detection (ECCV2024) Summary https://github.com/jbwang1997/StabilityIndex from Nankai University + KargoBot Inc. （自動運転企業） 3D detectionにおける時系列の安定性を考慮したMetrics、Stability Index (SI) の提案</description>
    </item>
    
    <item>
      <title>UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000993_unitraj/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000993_unitraj/</guid>
      <description>UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction (ECCV2024) Summary https://vita-epfl.github.io/UniTraj/ https://github.com/vita-epfl/UniTraj prediction taskを統一的に扱えるフレームワークの提案 巨大なデータでpredictionを学習したら、結局データの大きさが</description>
    </item>
    
    <item>
      <title>3D Small Object Detection with Dynamic Spatial Pruning (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000951_dspdet3d/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000951_dspdet3d/</guid>
      <description>3D Small Object Detection with Dynamic Spatial Pruning (ECCV2024) Summary https://xuxw98.github.io/DSPDet3D/ PruningしながらFPNする機構を備えたsmall object 3D detection real-time object detectionも考慮 https://github.com/xuxw98/DSPDet3D mmdet base https://www.youtube.com/watch?v=Wq-cIRnKhw0 Method Pruning 全体 Experiment Discussion</description>
    </item>
    
    <item>
      <title>Find n’ Propagate: Open-Vocabulary 3D Object Detection in Urban Environments (arxiv 2024/03, ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000846_findn_propagate/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000846_findn_propagate/</guid>
      <description>Find n’ Propagate: Open-Vocabulary 3D Object Detection in Urban Environments (arxiv 2024/03, ECCV2024) Summary Open-Vocabulary 3D Object Detection https://github.com/djamahl99/findnpropagate contribute 2D VLM を用いたfrustum base手法 Greedy Box Seeker frustumからsegmentしてspaceをsea</description>
    </item>
    
    <item>
      <title>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000942_vg_w3d/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000942_vg_w3d/</guid>
      <description>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance (ECCV2024) Summary from google 2D labelだけで3D detectionをするframeworkの提案 画像の特徴量を3D側に伝える工夫 https://github.com/kuanchihhuang/VG-W3D Method Background 3D Object Detection</description>
    </item>
    
    <item>
      <title>MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection (ICLR2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000928_mixsup/</link>
      <pubDate>Sun, 11 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000928_mixsup/</guid>
      <description>MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection (ICLR2024) Summary MixSupを使ったsemantic point clustersの利用 + PointSAM(3D Panoptic segmentation)使ったpseudo label による</description>
    </item>
    
    <item>
      <title>FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation (arxiv2023/12)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000862_frnet/</link>
      <pubDate>Sat, 10 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000862_frnet/</guid>
      <description>FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation (arxiv2023/12) Summary https://github.com/Xiangxu-0103/FRNet https://xiangxu-0103.github.io/FRNet Frustum basedな 3D semantic segmentation Method KNN post-processing の差 Experiment 可視化 https://www.youtube.com/watch?v=PvmnaMKnZrc https://www.youtube.com/watch?v=4m5sG-XsYgw https://www.youtube.com/watch?v=-aM_NaZLP8M incorrectの量が減っている score Semi-supervised: 少ないデータセットでもちゃんと上手く</description>
    </item>
    
    <item>
      <title>A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook (arxiv2024/01)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000843_autonomous_driving_dataset_survey/</link>
      <pubDate>Fri, 02 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000843_autonomous_driving_dataset_survey/</guid>
      <description>A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook (arxiv2024/01) Summary https://github.com/daniel-bogdoll/ad-datasets 元になったrepository Method 結構偏りが激しいし、結局nuScenesが良さげに見える task一覧としてわかり</description>
    </item>
    
    <item>
      <title>DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars (IROS2022)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000709_deepfusion_bosch/</link>
      <pubDate>Mon, 19 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000709_deepfusion_bosch/</guid>
      <description>DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars (IROS2022) Summary moduleのように扱える BEV baseのCamera-LiDAR-Radar fusion 3d detection 定性評価で細かく解析 LiDAR only だと縦</description>
    </item>
    
    <item>
      <title>aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception (arxiv 2022/11)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000689_aimotive_dataset/</link>
      <pubDate>Fri, 16 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000689_aimotive_dataset/</guid>
      <description>aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception (arxiv 2022/11) Summary https://arxiv.org/pdf/2211.09445.pdf https://github.com/aimotive/aimotive_dataset/tree/f71828446692587318ebccbd3cdad5b4335eb9f3 datasetに関して https://github.com/aimotive/aimotive-dataset-loader api https://github.com/aimotive/mm_training Camera-LiDAR-Radar dataset の提案 200mまでannotationされているので遠距離detectio</description>
    </item>
    
    <item>
      <title>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception? (arxiv 2022/09)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000683_simple_bev/</link>
      <pubDate>Tue, 13 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000683_simple_bev/</guid>
      <description>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception? (arxiv 2022/09) Summary https://simple-bev.github.io/ https://github.com/aharley/simple_bev NuScenes, Lyft でtrain code NuScenesはpretrain modelあり Camera-Radar fusion の BEV detection 検出 input: Camera * 6 (360度) + radar pointcloud Depth-based, Homography-based では</description>
    </item>
    
    <item>
      <title>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation (arxiv2022/05)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000606_bev_fusion/</link>
      <pubDate>Sat, 10 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000606_bev_fusion/</guid>
      <description>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation (arxiv2022/05) Summary https://bevfusion.mit.edu/ 公式 https://github.com/mit-han-lab/bevfusion mmdet base、waymo, nuscenes で評価 pretrained modelがある https://www.youtube.com/watch?v=uCAka90si9E BEV特徴量空間でfusionするCam</description>
    </item>
    
    <item>
      <title>Center-based 3D Object Detection and Tracking (araxiv 2020/06, CVPR2021)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000411_centerpoint/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000411_centerpoint/</guid>
      <description>summary LiDAR-based 3d object detection + tracking Anchor-free 2020年あたりからのデファクトスタンダード https://github.com/tianweiy/CenterPoint github Centerの点を考える手法 anchor-freeで考えられて、tracki</description>
    </item>
    
    <item>
      <title>EagerMOT: 3D Multi-Object Tracking via Sensor Fusion (ICRA2021)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000298_eager_mot/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000298_eager_mot/</guid>
      <description>summary Cameraだけ/LiDARだけ/Camera-Lidarに対応している Sensor FusionのTracking フレームワーク (i) fusion of 3D and 2D evidence that merges detections</description>
    </item>
    
    <item>
      <title>LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic Segmentation Based on End-to-End-Learned 3D Features and U-Net (ICCV Workshop 2019)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000307_lunet/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000307_lunet/</guid>
      <description>概要 https://arxiv.org/abs/1908.11656 https://github.com/pbias/lunet github Lidar 3D Semantic Segmentation 3D -&amp;gt; 2D -&amp;gt; Unet range imageにおけるSegmantationのわかりやすい概要図がある range image baseを割合細かく説明されている印象</description>
    </item>
    
    <item>
      <title>Multi-View 3D Object Detection Network for Autonomous Driving (IROS2019)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000304_rangenet_pp/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000304_rangenet_pp/</guid>
      <description>概要 Rangenet++の提案 https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/milioto2019iros.pdf Paper https://www.youtube.com/watch?v=wuokg7MFZyU youtube http://jbehley.github.io/ 著者 https://github.com/PRBonn/lidar-bonnetal github Lidar-only semantic segmentation. Lidarのreal-time procesing, 10fps程度 projection-based methods a spherical projectionを使った2D</description>
    </item>
    
    <item>
      <title>MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000295_mvlidarnet_nvidia/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000295_mvlidarnet_nvidia/</guid>
      <description>Summary Link https://arxiv.org/abs/2006.05518 arxiv版 https://www.youtube.com/watch?v=2ck5_sToayc https://www.youtube.com/watch?v=T7w-ZCVVUgM https://blogs.nvidia.com/blog/2020/03/11/drive-labs-multi-view-lidarnet-self-driving-cars/ nvidia blog githubはない 2020/10現在 Two-stage型 Lidar 3d multi-class detection framework “multi-view” = “perspecti</description>
    </item>
    
    <item>
      <title>One Million Scenes for Autonomous Driving: ONCE Dataset (2021/06 arxiv)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000299_once_dataset/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000299_once_dataset/</guid>
      <description>summary ONCE (One millioN sCenEs) dataset Huaweiから出たDataset baselibeについて色々結果をまとめているのでsurvey記事として価値が高い baseli</description>
    </item>
    
    <item>
      <title>Optimising the selection of samples for robust lidar camera calibration (arxiv 2021/03)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000356_camera_lidar_calib/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000356_camera_lidar_calib/</guid>
      <description>Summary 使いやすい形になっているっぽい https://gitlab.acfr.usyd.edu.au/its/cam_lidar_calibration https://www.youtube.com/watch?v=WmzEnjmffQU 素人でもcalibrationができるようなtarget-based なLidar- Camera calibのパイプラ</description>
    </item>
    
  </channel>
</rss>
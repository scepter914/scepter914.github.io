<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Surveys on Scepter914 Website</title>
    <link>https://scepter914.github.io/survey/</link>
    <description>Recent content in Surveys on Scepter914 Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Jan 2025 10:00:00 +0900</lastBuildDate><atom:link href="https://scepter914.github.io/survey/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DRIVE VLM: The Convergence of Autonomous Driving and Large Vision-Language Models (arxiv2024/02, CoRL2024)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000981_drivevlm/</link>
      <pubDate>Thu, 02 Jan 2025 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000981_drivevlm/</guid>
      <description>DRIVE VLM: The Convergence of Autonomous Driving and Large Vision-Language Models (arxiv2024/02, CoRL2024) Summary https://github.com/Tsinghua-MARS-Lab/DriveVLM 2024/10/17現在未公開 https://tsinghua-mars-lab.github.io/DriveVLM/ Method DriveVLM-Dual architecture Output Meta-action Decision Waypoints Traditional pipeline = E2E model のこと Integrating 3D Perception. 2Dに投影して、critical objec</description>
    </item>
    
    <item>
      <title>FutureMotion (2024/05 github)</title>
      <link>https://scepter914.github.io/survey/perception/planning/001095_future_motion/</link>
      <pubDate>Thu, 02 Jan 2025 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/001095_future_motion/</guid>
      <description>FutureMotion (2024/05 github) Summary https://github.com/kit-mrt/future-motion かなりちゃんと書かれている predictionのlibrary Method forward 見ると大体分かる inputがかなり抽象化されている class Wayformer(nn.Module): def forward( self, target_valid: Tensor,</description>
    </item>
    
    <item>
      <title>Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding (NeurIPS 2023)</title>
      <link>https://scepter914.github.io/survey/perception/planning/001096_hptr/</link>
      <pubDate>Thu, 02 Jan 2025 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/001096_hptr/</guid>
      <description>Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding (NeurIPS 2023) Summary [[001095_future_motion]] の元になった論文 scene-centricで計算効率をよく、agent-centricで性能よく、を合体させた</description>
    </item>
    
    <item>
      <title>Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving (arxiv 2024/10)</title>
      <link>https://scepter914.github.io/survey/perception/planning/001034_senna/</link>
      <pubDate>Thu, 02 Jan 2025 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/001034_senna/</guid>
      <description>Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving (arxiv 2024/10) Summary VLMとE2E modelを繋ぐフレームワーク Senna を用いた自動運転 DriveVLMの後継 Method Experiment DriveVLM とはそんなに変わらないかな</description>
    </item>
    
    <item>
      <title>Far3D: Expanding the Horizon for Surround-View 3D Object Detection (AAAI2024)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000852_far3d/</link>
      <pubDate>Sun, 22 Dec 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000852_far3d/</guid>
      <description>Far3D: Expanding the Horizon for Surround-View 3D Object Detection (AAAI2024) Summary https://github.com/megvii-research/Far3D multi-camera 3D detection for far detection Method 遠距離はCamera onlyの方が精度良い frustumにfeatureを散布する 全体アーキテクチャ Experiment</description>
    </item>
    
    <item>
      <title>RegTTA3D: Better Regression Makes Better Test-time Adaptive 3D Object Detection (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/001008_regtta3d/</link>
      <pubDate>Thu, 14 Nov 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/001008_regtta3d/</guid>
      <description>RegTTA3D: Better Regression Makes Better Test-time Adaptive 3D Object Detection (ECCV2024) Summary Test-time adaptationを用いた3D detection Domain adaptation Regressionを中心に少ないparameterでtuningできる Method Domain</description>
    </item>
    
    <item>
      <title>CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000992_catsam/</link>
      <pubDate>Fri, 01 Nov 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000992_catsam/</guid>
      <description>CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model (ECCV2024) Summary https://xiaoaoran.github.io/projects/CAT-SAM https://github.com/weihao1115/cat-sam &amp;ldquo;a ConditionAl Tuning network&amp;rdquo; for SAM SAMに additional pipelineを追加して、元のSAMはfrozenしてadaptationする研究 Method architecture a</description>
    </item>
    
    <item>
      <title>Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000972_detecting_as_labeling/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000972_detecting_as_labeling/</guid>
      <description>Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection (ECCV2024) Summary from Phigent Robotics CTOが Baidu -&amp;gt; Horizon Robotics の経歴 https://github.com/HuangJunJie2017/BEVDet Camera LiDAR 3D detection において、Camera pipelineはlabel推定にしか使わないようにした</description>
    </item>
    
    <item>
      <title>On-the-fly Category Discovery for LiDAR Semantic Segmentation (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/001003_ocdss/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/001003_ocdss/</guid>
      <description>On-the-fly Category Discovery for LiDAR Semantic Segmentation (ECCV2024) Summary Unknown objectのための LiDAR Semantic Segmentationに必要なカテゴライズの導入 Method Baseは &amp;ldquo;On-the-fly Category Discovery (CVPR 2023)&amp;rdquo; https://github.com/PRIS-CV/On-the-fly-Category-Discovery 解きたいタスクの違い (a)</description>
    </item>
    
    <item>
      <title>Towards Stable 3D Object Detection (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000969_towards_stable/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000969_towards_stable/</guid>
      <description>Towards Stable 3D Object Detection (ECCV2024) Summary https://github.com/jbwang1997/StabilityIndex from Nankai University + KargoBot Inc. （自動運転企業） 3D detectionにおける時系列の安定性を考慮したMetrics、Stability Index (SI) の提案</description>
    </item>
    
    <item>
      <title>UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000993_unitraj/</link>
      <pubDate>Sun, 27 Oct 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000993_unitraj/</guid>
      <description>UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction (ECCV2024) Summary https://vita-epfl.github.io/UniTraj/ https://github.com/vita-epfl/UniTraj prediction taskを統一的に扱えるフレームワークの提案 巨大なデータでpredictionを学習したら、結局データの大きさが</description>
    </item>
    
    <item>
      <title>3D Small Object Detection with Dynamic Spatial Pruning (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000951_dspdet3d/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000951_dspdet3d/</guid>
      <description>3D Small Object Detection with Dynamic Spatial Pruning (ECCV2024) Summary https://xuxw98.github.io/DSPDet3D/ PruningしながらFPNする機構を備えたsmall object 3D detection real-time object detectionも考慮 https://github.com/xuxw98/DSPDet3D mmdet base https://www.youtube.com/watch?v=Wq-cIRnKhw0 Method Pruning 全体 Experiment Discussion</description>
    </item>
    
    <item>
      <title>Find n’ Propagate: Open-Vocabulary 3D Object Detection in Urban Environments (arxiv 2024/03, ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000846_findn_propagate/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000846_findn_propagate/</guid>
      <description>Find n’ Propagate: Open-Vocabulary 3D Object Detection in Urban Environments (arxiv 2024/03, ECCV2024) Summary Open-Vocabulary 3D Object Detection https://github.com/djamahl99/findnpropagate contribute 2D VLM を用いたfrustum base手法 Greedy Box Seeker frustumからsegmentしてspaceをsea</description>
    </item>
    
    <item>
      <title>Injecting Planning-Awareness into Prediction and Detection Evaluation (IV2022)</title>
      <link>https://scepter914.github.io/survey/perception/evaluation/000558_pi_metrics/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/evaluation/000558_pi_metrics/</guid>
      <description>Injecting Planning-Awareness into Prediction and Detection Evaluation (IV2022) Summary https://github.com/BorisIvanovic/PlanningAwareEvaluation Detectionとforecastingに使えるPI-metricsの提案 Method 他の手法 task-aware evaluation metrics be: Able to capture asymmetries in downstream tasks. Method-agnostic. Computationally feasible to compute. Interpretable</description>
    </item>
    
    <item>
      <title>Soft Robotics Commercialization: Jamming Grippers from Research to Product (Soft robotics 2016)</title>
      <link>https://scepter914.github.io/survey/robotics/000838_jamming_gripper/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/robotics/000838_jamming_gripper/</guid>
      <description>Summary ドラえもんハンドのproduct化で大変だったことまとめ集 ソフトロボティクスを事業化しようとするなら必読 動画 https://www.youtube.com/watch?v=GdJyICIp4t4 https://www.youtube.com/watch?v=KZ0Y2fDZ8Uw https://www.youtube.com/watch?v=zgHSAUEzjn4 Background これまでの世界にimp</description>
    </item>
    
    <item>
      <title>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance (ECCV2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000942_vg_w3d/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000942_vg_w3d/</guid>
      <description>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance (ECCV2024) Summary from google 2D labelだけで3D detectionをするframeworkの提案 画像の特徴量を3D側に伝える工夫 https://github.com/kuanchihhuang/VG-W3D Method Background 3D Object Detection</description>
    </item>
    
    <item>
      <title>What data do we need for training an AV motion planner? (ICRA2021)</title>
      <link>https://scepter914.github.io/survey/perception/evaluation/000575_data_for_planning/</link>
      <pubDate>Mon, 12 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/evaluation/000575_data_for_planning/</guid>
      <description>What data do we need for training an AV motion planner? (ICRA2021) Summary from Lyft どういうperceptionのobjectが影響しているのかを解析した論文 Method data種類 Range and field-of-view Geometric accuracy Addressing domain shift Experiment data fine</description>
    </item>
    
    <item>
      <title>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data (arxiv 2024/01)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000817_depth_anything/</link>
      <pubDate>Sun, 11 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000817_depth_anything/</guid>
      <description>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data (arxiv 2024/01) Summary https://github.com/LiheYoung/Depth-Anything link https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main model https://github.com/spacewalk01/depth-anything-tensorrt TensorRT実装 from TikTok 汎用性のある Zero-shot monocular relative depth estimation 基盤model + Unsupervisedな学習の手法を取り入</description>
    </item>
    
    <item>
      <title>Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection (ECCV2023)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000959_stream_petr/</link>
      <pubDate>Sun, 11 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000959_stream_petr/</guid>
      <description>Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection (ECCV2023) Summary Multi-Camera 3D detection 実行速度も早く、multi-Camera 3D detection の中だと今一番強いまである https://github.com/exiawsh/StreamPETR/ https://github.com/NVIDIA/DL4AGX/tree/master/AV-Solutions/streampetr-trt TensorRT実装もある Method Experiment RTX3090 で動</description>
    </item>
    
    <item>
      <title>MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection (ICLR2024)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000928_mixsup/</link>
      <pubDate>Sun, 11 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000928_mixsup/</guid>
      <description>MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection (ICLR2024) Summary MixSupを使ったsemantic point clustersの利用 + PointSAM(3D Panoptic segmentation)使ったpseudo label による</description>
    </item>
    
    <item>
      <title>OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples (IEEE Transaction on visualization and computer graphics 2024/01)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000895_ow_adapter/</link>
      <pubDate>Sun, 11 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000895_ow_adapter/</guid>
      <description>OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples (IEEE Transaction on visualization and computer graphics 2024/01) Summary https://www.youtube.com/watch?v=QNub6PYMp1k Method 非常にイケているUI 所謂 pseudo label の 分布の表現も必要っぽい Experiment Discussion</description>
    </item>
    
    <item>
      <title>SAM 2: Segment Anything in Images and Videos (arxiv 2024/07)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000907_sam_v2/</link>
      <pubDate>Sun, 11 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000907_sam_v2/</guid>
      <description>SAM 2: Segment Anything in Images and Videos (arxiv 2024/07) Summary https://github.com/facebookresearch/segment-anything-2 参考 https://speakerdeck.com/tenten0727/segment-anything-model-2?slide=5 Method dataset Experiment 高速化 FPSが結構出てるのすごい Model Size (M) Speed (FPS) SA-V test (J&amp;amp;F) MOSE val (J&amp;amp;F) LVOS v2 (J&amp;amp;F) sam2_hiera_tiny 38.9 47.2 75.0 70.9 75.3 sam2_hiera_small 46 43.3 (53.0 compiled*) 74.9 71.5 76.4 sam2_hiera_base_plus 80.8 34.8 (43.8 compiled*) 74.7 72.8 75.8</description>
    </item>
    
    <item>
      <title>FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation (arxiv2023/12)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000862_frnet/</link>
      <pubDate>Sat, 10 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000862_frnet/</guid>
      <description>FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation (arxiv2023/12) Summary https://github.com/Xiangxu-0103/FRNet https://xiangxu-0103.github.io/FRNet Frustum basedな 3D semantic segmentation Method KNN post-processing の差 Experiment 可視化 https://www.youtube.com/watch?v=PvmnaMKnZrc https://www.youtube.com/watch?v=4m5sG-XsYgw https://www.youtube.com/watch?v=-aM_NaZLP8M incorrectの量が減っている score Semi-supervised: 少ないデータセットでもちゃんと上手く</description>
    </item>
    
    <item>
      <title>SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving (arxiv2024/02, RA-L 2024)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000835_simpl/</link>
      <pubDate>Sun, 04 Aug 2024 10:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000835_simpl/</guid>
      <description>SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving (arxiv2024/02, RA-L 2024) Summary https://www.youtube.com/watch?v=_8-6ccopZMM https://github.com/HKUST-Aerial-Robotics/SIMPL?tab=readme-ov-file Transformer base prediction Method Experiment 3060Ti 250Actorでも20ms程度はめっちゃ使い勝手は良さそう 可視化の動画めっちゃ良さげに見え</description>
    </item>
    
    <item>
      <title>A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook (arxiv2024/01)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000843_autonomous_driving_dataset_survey/</link>
      <pubDate>Fri, 02 Aug 2024 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000843_autonomous_driving_dataset_survey/</guid>
      <description>A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook (arxiv2024/01) Summary https://github.com/daniel-bogdoll/ad-datasets 元になったrepository Method 結構偏りが激しいし、結局nuScenesが良さげに見える task一覧としてわかり</description>
    </item>
    
    <item>
      <title>MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception (arxiv2022/11)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000721_matrixvt/</link>
      <pubDate>Tue, 20 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000721_matrixvt/</guid>
      <description>MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception (arxiv2022/11) Summary https://github.com/Megvii-BaseDetection/BEVDepth BEVDepthの後継、軽量version 軽量なBEV-base Camera 3d detection CPUでも動作するレベルで軽量 CPUでも数10</description>
    </item>
    
    <item>
      <title>DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars (IROS2022)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000709_deepfusion_bosch/</link>
      <pubDate>Mon, 19 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000709_deepfusion_bosch/</guid>
      <description>DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars (IROS2022) Summary moduleのように扱える BEV baseのCamera-LiDAR-Radar fusion 3d detection 定性評価で細かく解析 LiDAR only だと縦</description>
    </item>
    
    <item>
      <title>aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception (arxiv 2022/11)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000689_aimotive_dataset/</link>
      <pubDate>Fri, 16 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000689_aimotive_dataset/</guid>
      <description>aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception (arxiv 2022/11) Summary https://arxiv.org/pdf/2211.09445.pdf https://github.com/aimotive/aimotive_dataset/tree/f71828446692587318ebccbd3cdad5b4335eb9f3 datasetに関して https://github.com/aimotive/aimotive-dataset-loader api https://github.com/aimotive/mm_training Camera-LiDAR-Radar dataset の提案 200mまでannotationされているので遠距離detectio</description>
    </item>
    
    <item>
      <title>MTP: Multi-hypothesis Tracking and Prediction for Reduced Error Propagation (IV2022)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000560_mtp/</link>
      <pubDate>Thu, 15 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000560_mtp/</guid>
      <description>MTP: Multi-hypothesis Tracking and Prediction for Reduced Error Propagation (IV2022) Summary from Carnegie mellon and nvidia https://www.youtube.com/watch?v=ydQ9IPbX_-A multi-hypothesis tracking and prediction framework の提案 tracking results を複数持つことでpredictionの性能を上げる tracking errors が prediction performance に与える影響の解析も行って</description>
    </item>
    
    <item>
      <title>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception? (arxiv 2022/09)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000683_simple_bev/</link>
      <pubDate>Tue, 13 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000683_simple_bev/</guid>
      <description>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception? (arxiv 2022/09) Summary https://simple-bev.github.io/ https://github.com/aharley/simple_bev NuScenes, Lyft でtrain code NuScenesはpretrain modelあり Camera-Radar fusion の BEV detection 検出 input: Camera * 6 (360度) + radar pointcloud Depth-based, Homography-based では</description>
    </item>
    
    <item>
      <title>BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving (arxiv2022/05)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000707_beverse/</link>
      <pubDate>Sun, 11 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000707_beverse/</guid>
      <description>Summary https://arxiv.org/pdf/2205.09743.pdf https://github.com/zhangyp15/BEVerse Multi-Camera BEV perception 3d detection, motion prediction, semantic map のmulti-task learning semantic map は mapのみでobjectは含まない motion prediction はsegmentationの表現で行われている Multi-frame,</description>
    </item>
    
    <item>
      <title>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers (ECCV2022)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000692_bev_former/</link>
      <pubDate>Sun, 11 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000692_bev_former/</guid>
      <description>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers (ECCV2022) Summary https://github.com/fundamentalvision/BEVFormer Attention base でのMulti-cameraからBird&amp;rsquo;s-Eye-View Representation を得る</description>
    </item>
    
    <item>
      <title>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation (arxiv2022/05)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000606_bev_fusion/</link>
      <pubDate>Sat, 10 Dec 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000606_bev_fusion/</guid>
      <description>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation (arxiv2022/05) Summary https://bevfusion.mit.edu/ 公式 https://github.com/mit-han-lab/bevfusion mmdet base、waymo, nuscenes で評価 pretrained modelがある https://www.youtube.com/watch?v=uCAka90si9E BEV特徴量空間でfusionするCam</description>
    </item>
    
    <item>
      <title>MTBF Model for AVs - From Perception Errors to Vehicle-Level Failures (IV2022)</title>
      <link>https://scepter914.github.io/survey/perception/evaluation/000543_mtbf/</link>
      <pubDate>Tue, 21 Jun 2022 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/evaluation/000543_mtbf/</guid>
      <description>MTBF Model for AVs - From Perception Errors to Vehicle-Level Failures (IV2022) Summary from Intel + Mobileye RSS modelはPerception errorを考慮していなかったため、perception system error と vehicle-level failures (= collision) を</description>
    </item>
    
    <item>
      <title>Center-based 3D Object Detection and Tracking (araxiv 2020/06, CVPR2021)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000411_centerpoint/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000411_centerpoint/</guid>
      <description>summary LiDAR-based 3d object detection + tracking Anchor-free 2020年あたりからのデファクトスタンダード https://github.com/tianweiy/CenterPoint github Centerの点を考える手法 anchor-freeで考えられて、tracki</description>
    </item>
    
    <item>
      <title>EagerMOT: 3D Multi-Object Tracking via Sensor Fusion (ICRA2021)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000298_eager_mot/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000298_eager_mot/</guid>
      <description>summary Cameraだけ/LiDARだけ/Camera-Lidarに対応している Sensor FusionのTracking フレームワーク (i) fusion of 3D and 2D evidence that merges detections</description>
    </item>
    
    <item>
      <title>Investigating the Effect of Sensor Modalities in Multi-Sensor Detection-Prediction Models (NeurIPS 2020 Workshop)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000297_sensor_dropout/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000297_sensor_dropout/</guid>
      <description>summary sensor dropoutを用いて Detection + PredictionにおけるSensorのcontriobutionを解析 from Uber Model 2つの合体 RV-MultiXNet (S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C.</description>
    </item>
    
    <item>
      <title>Let’s Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving (WACV2021)</title>
      <link>https://scepter914.github.io/survey/perception/vision/000345_lets_dirty/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/vision/000345_lets_dirty/</guid>
      <description>Summary https://openaccess.thecvf.com/content/WACV2021/papers/Uricar_Lets_Get_Dirty_GAN_Based_Data_Augmentation_for_Camera_Lens_WACV_2021_paper.pdf レンズの汚れをマスクしたデータセットを公開する予定 https://github.com/uricamic/soiling 2021/02/06地点で未公開 魚眼レンズの汚れたデータ・セット あるデータ・セットに</description>
    </item>
    
    <item>
      <title>LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic Segmentation Based on End-to-End-Learned 3D Features and U-Net (ICCV Workshop 2019)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000307_lunet/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000307_lunet/</guid>
      <description>概要 https://arxiv.org/abs/1908.11656 https://github.com/pbias/lunet github Lidar 3D Semantic Segmentation 3D -&amp;gt; 2D -&amp;gt; Unet range imageにおけるSegmantationのわかりやすい概要図がある range image baseを割合細かく説明されている印象</description>
    </item>
    
    <item>
      <title>Multi-View 3D Object Detection Network for Autonomous Driving (IROS2019)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000304_rangenet_pp/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000304_rangenet_pp/</guid>
      <description>概要 Rangenet++の提案 https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/milioto2019iros.pdf Paper https://www.youtube.com/watch?v=wuokg7MFZyU youtube http://jbehley.github.io/ 著者 https://github.com/PRBonn/lidar-bonnetal github Lidar-only semantic segmentation. Lidarのreal-time procesing, 10fps程度 projection-based methods a spherical projectionを使った2D</description>
    </item>
    
    <item>
      <title>Multimodal Trajectory Predictions for Autonomous Driving using Deep Convolutional Networks (ICRA2019)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000361_uber_prediction_mtp/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000361_uber_prediction_mtp/</guid>
      <description>Summary Raster mapを用いた1 agent multi-modal prediction https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/prediction/models/mtp.py github code 確率とともに複数経路(= multi-modal) を出力できるように拡張 single modalだと行きもしないところのpathが出てくる Method i: a</description>
    </item>
    
    <item>
      <title>Multiple Trajectory Prediction with Deep Temporal and Spatial Convolutional Neural Networks (IROS2020)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000448_tcn_prediction/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000448_tcn_prediction/</guid>
      <description>summary temporal convolutional networks (TCNs) を用いたPredictionの提案 Method 軽くて良さげな全体framework mobilenetなのがrealtimeを考慮していて良い</description>
    </item>
    
    <item>
      <title>MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000295_mvlidarnet_nvidia/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000295_mvlidarnet_nvidia/</guid>
      <description>Summary Link https://arxiv.org/abs/2006.05518 arxiv版 https://www.youtube.com/watch?v=2ck5_sToayc https://www.youtube.com/watch?v=T7w-ZCVVUgM https://blogs.nvidia.com/blog/2020/03/11/drive-labs-multi-view-lidarnet-self-driving-cars/ nvidia blog githubはない 2020/10現在 Two-stage型 Lidar 3d multi-class detection framework “multi-view” = “perspecti</description>
    </item>
    
    <item>
      <title>One Million Scenes for Autonomous Driving: ONCE Dataset (2021/06 arxiv)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000299_once_dataset/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000299_once_dataset/</guid>
      <description>summary ONCE (One millioN sCenEs) dataset Huaweiから出たDataset baselibeについて色々結果をまとめているのでsurvey記事として価値が高い baseli</description>
    </item>
    
    <item>
      <title>Optimising the selection of samples for robust lidar camera calibration (arxiv 2021/03)</title>
      <link>https://scepter914.github.io/survey/perception/detection3d/000356_camera_lidar_calib/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/detection3d/000356_camera_lidar_calib/</guid>
      <description>Summary 使いやすい形になっているっぽい https://gitlab.acfr.usyd.edu.au/its/cam_lidar_calibration https://www.youtube.com/watch?v=WmzEnjmffQU 素人でもcalibrationができるようなtarget-based なLidar- Camera calibのパイプラ</description>
    </item>
    
    <item>
      <title>The Importance of Prior Knowledge in Precise Multimodal Prediction (IROS2020)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000444_uber_reinforce/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000444_uber_reinforce/</guid>
      <description>summary UberによるPrediction framework REINFORCE の提案 Mapや交通ルールから好ましい予測のoutputを出したいが、基本的に微分不可能 REINFORCE gradient estim</description>
    </item>
    
    <item>
      <title>Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving (arxiv2018, WACV2020)</title>
      <link>https://scepter914.github.io/survey/perception/planning/000360_uber_prediction/</link>
      <pubDate>Sun, 10 Oct 2021 13:00:00 +0900</pubDate>
      
      <guid>https://scepter914.github.io/survey/perception/planning/000360_uber_prediction/</guid>
      <description>Summary from Uber 1 agentごとの静的地図+他の車両など動的状況をラスタライズ化した画像をinputとした1 agent, single-modal prediction Background Method Input Rasterized input RGBの3chのデータ 道路</description>
    </item>
    
  </channel>
</rss>
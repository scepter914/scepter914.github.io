














<!DOCTYPE html>
<html lang='en-us'><head>
    <meta charset="utf-8">
    <link rel="shortcut icon" href='https://scepter914.github.io/favicon.ico' type="image/x-icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020) - Scepter914 Website</title>

    

    

    

    
        <meta property="og:title" content="MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020)" />
<meta property="og:description" content="Summary Link https://arxiv.org/abs/2006.05518 arxiv版 https://www.youtube.com/watch?v=2ck5_sToayc https://www.youtube.com/watch?v=T7w-ZCVVUgM https://blogs.nvidia.com/blog/2020/03/11/drive-labs-multi-view-lidarnet-self-driving-cars/ nvidia blog githubはない 2020/10現在 Two-stage型 Lidar 3d multi-class detection framework “multi-view” = “perspecti" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://scepter914.github.io/survey/perception/detection3d/000295_mvlidarnet_nvidia/" /><meta property="article:section" content="survey" />
<meta property="article:published_time" content="2021-10-10T13:00:00+09:00" />
<meta property="article:modified_time" content="2021-10-10T13:00:00+09:00" />


    

    
        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020)"/>
<meta name="twitter:description" content="Summary Link https://arxiv.org/abs/2006.05518 arxiv版 https://www.youtube.com/watch?v=2ck5_sToayc https://www.youtube.com/watch?v=T7w-ZCVVUgM https://blogs.nvidia.com/blog/2020/03/11/drive-labs-multi-view-lidarnet-self-driving-cars/ nvidia blog githubはない 2020/10現在 Two-stage型 Lidar 3d multi-class detection framework “multi-view” = “perspecti"/>

    <link rel="stylesheet" href="https://scepter914.github.io/style.min.5297c96c59a52afaa5bcda4a6cedf3813081f64025c209b25b2ee6d0c8f74d462b625ad3404a92a14d7a51b4ec0a420337ae70f426fa4bce2d5f7459a3ca7274.css" integrity="sha512-UpfJbFmlKvqlvNpKbO3zgTCB9kAlwgmyWy7m0Mj3TUYrYlrTQEqSoU16UbTsCkIDN65w9Cb6S84tX3RZo8pydA==">





    
    <script>
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.setAttribute("data-theme", "dark");
        } else {
            document.documentElement.setAttribute("data-theme", "light");
        }
    </script>
<script defer src="https://scepter914.github.io/js/header.7a2a109ec3782c57bad0332b662f8a5f41765505936b69868eb8bd5241de9daf23c388e82ca1831f6d09935013dcb9f71bfa7face3975880c1076028b7b0a6e1.js" integrity="sha512-eioQnsN4LFe60DMrZi&#43;KX0F2VQWTa2mGjri9UkHena8jw4joLKGDH20Jk1AT3Ln3G/p/rOOXWIDBB2Aot7Cm4Q=="></script>



    <script defer src="https://scepter914.github.io/js/zooming.fc76b730bcba24949cb337c58c5e935f727dbea8418e918e3b1794f0707d130aadf4ead341e9305a2c6045f7bc66700d73cc5c7f5d8f6ae2c7631e095f4e18ae.js" integrity="sha512-/Ha3MLy6JJScszfFjF6TX3J9vqhBjpGOOxeU8HB9Ewqt9OrTQekwWixgRfe8ZnANc8xcf12PauLHYx4JX04Yrg=="></script>




    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script defer src="https://scepter914.github.io/js/math.d7efde37b2eb8879651e1f4489bcd4d8203b8c2bf8ca12c9e1b8cd11bfd6395b172f4999fff43ce0d047889a2bdb71ee74aebbae5327590192d1144e790fcd7b.js" integrity="sha512-1&#43;/eN7LriHllHh9EibzU2CA7jCv4yhLJ4bjNEb/WOVsXL0mZ//Q84NBHiJor23HudK67rlMnWQGS0RROeQ/New=="></script>









</head><body>
        <main><header>
    <div class="brand">
        <div id="sidebar_btn">
            <svg id="menu_icon" width="26px" height="26px" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></svg>
        </div>

        <div>
            <a href="https://scepter914.github.io/">Scepter914 Website</a>
        </div>
    </div>

    <div class="toolbox">
        <div id="theme_tool">
            <svg id="dark_mode_btn" class="hidden toolbox-btn" width="18px" height="18px" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></svg>
            <svg id="light_mode_btn" class="hidden toolbox-btn" width="18px" height="18px" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></svg>
        </div>

        

        
    </div>
</header>
<nav id="navbar" class="pure-menu"><ul class="pure-menu-list"><li class="navbar-item pure-menu-item ">
                    
                        <a href="https://scepter914.github.io/project/aboutmeja" class="pure-menu-link">About me (Japanese)</a>
                    
                </li><li class="navbar-item pure-menu-item ">
                    
                        <a href="https://scepter914.github.io/blog" class="pure-menu-link">Blog</a>
                    
                </li><li class="navbar-item pure-menu-item insection">
                    
                        <a href="https://scepter914.github.io/survey" class="pure-menu-link">Survey</a>
                    
                </li></ul>
</nav>
<div id="sidebar_canvas_overlay" class="hidden"></div>
<div id="sidebar" class="close">
    <ul><li>
                    <a href="https://scepter914.github.io/project/aboutmeja">About me (Japanese)</a>
                </li><li>
                    <a href="https://scepter914.github.io/blog">Blog</a>
                </li><li>
                    <a href="https://scepter914.github.io/survey">Survey</a>
                </li></ul>
</div><div id="content" class="content-margin">
                
    
    <div class="collapsible-menu-wrapper"><div class="collapsible-menu-type"><span>Table of contents</span></div><div class="collapsible-menu">
        
            <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#summary">Summary</a></li>
        <li><a href="#background">background</a></li>
        <li><a href="#method">Method</a></li>
        <li><a href="#experiment">Experiment</a></li>
        <li><a href="#discussion">Discussion</a></li>
      </ul>
    </li>
  </ul>
</nav>
        
    </div></div>



    <div class="content-margin">

<article class="line-numbers">
    
    
    <h3 id="summary">Summary</h3>
<ul>
<li>Link
<ul>
<li><a href="https://arxiv.org/abs/2006.05518">https://arxiv.org/abs/2006.05518</a> arxiv版</li>
<li><a href="https://www.youtube.com/watch?v=2ck5_sToayc">https://www.youtube.com/watch?v=2ck5_sToayc</a></li>
<li><a href="https://www.youtube.com/watch?v=T7w-ZCVVUgM">https://www.youtube.com/watch?v=T7w-ZCVVUgM</a></li>
<li><a href="https://blogs.nvidia.com/blog/2020/03/11/drive-labs-multi-view-lidarnet-self-driving-cars/">https://blogs.nvidia.com/blog/2020/03/11/drive-labs-multi-view-lidarnet-self-driving-cars/</a> nvidia blog</li>
<li>githubはない 2020/10現在</li>
</ul>
</li>
<li>Two-stage型 Lidar 3d multi-class detection framework
<ul>
<li>“multi-view” = “perspective view” + BEV</li>
<li>“perspective view” baseのsemantic segmentation : Range imageのこと
<ul>
<li>drivable spaceも計算できるsemantic segmentation</li>
</ul>
</li>
<li>BEV base のdetectction and classify objects</li>
<li>2D*2stageで3d detectionを行うframework</li>
</ul>
</li>
<li>100台以上の車両と歩行者がいるシーンでもLidar1つでdetectionを可能とした
<ul>
<li>embedded GPU上で150 fpsで動作</li>
</ul>
</li>
<li>拡張性が高いarchitectureで高速動作、非常に参考になる</li>
</ul>
<h3 id="background">background</h3>
<ul>
<li>3d 処理
<ul>
<li>3dのまま: 計算コスト大</li>
<li>BEV: 情報量落ちすぎて小さいobjectのdetectionができない</li>
</ul>
</li>
<li>Related works
<ul>
<li>BEV 3d detection: PIXOR</li>
<li>Voxel base 3d detection: VoxcelNet, SECOND</li>
<li>BEV LV fusion detecion: MV3D, AVOD</li>
<li>3D semantic segmentation: PointNet, STD</li>
<li>“End-to-end multi-view fusion for 3d object detection in lidar point clouds&quot;
<ul>
<li>multi view関連</li>
<li>別branchが走っていてデバッグが複雑</li>
<li>“In contrast, our approach uses explicit features and representations for perspective and topdown view, which makes the system easy to train and debug; our network performs multi-class object detection and is an order of magnitude faster”</li>
</ul>
</li>
<li>RangeBase semantic segmentation RangeNet++</li>
</ul>
</li>
</ul>
<h3 id="method">Method</h3>
<p><img src="https://scepter914.github.io/uploads/r/mvlidarnet_1.png" alt=""></p>
<ul>
<li>multi-classなのでclassごとの学習は要らない</li>
<li>7 classes: cars, trucks, pedestrians, cyclists, road surface, sidewalks, and unknown
<ul>
<li>“We experimented with more or fewer classes but found the best results were obtained with this choice”</li>
</ul>
</li>
</ul>
<h4 id="segmentation">Segmentation</h4>
<p><img src="https://scepter914.github.io/uploads/r/mvlidarnet_3.png" alt=""></p>
<ul>
<li>input: Lidar range image (3, 64, 2048)
<ul>
<li>(点の有無、depth, intensity)？</li>
<li>点の有無についての言及がない？
<ul>
<li>図を見ると欠損値自体はある</li>
<li>[[stdcv_00032_baidu_seg]] とか考えるとそうじゃない？</li>
</ul>
</li>
</ul>
</li>
<li>output: 7 class semantic image (7, 64, 2048)
<ul>
<li>drivable spaceも出力される</li>
</ul>
</li>
</ul>
<h4 id="bev-detection">BEV detection</h4>
<ul>
<li>input:
<ul>
<li>BEV semantic image (7, 1024, 1024)</li>
<li>BEV lidar feature (min height, max height, mean intensity) (3, 1024, 1024)</li>
</ul>
</li>
<li>network output
<ul>
<li>classhead 3class (3, 256, 256)</li>
<li>bboxhead3 (6, 256, 256)
<ul>
<li>(δx, δy) 重心, (wo, lo) object size , (sin θ, cos θ) yaw</li>
</ul>
</li>
</ul>
</li>
<li>final output
<ul>
<li>class, (δx, δy) 重心, (wo, lo) object size , (sin θ, cos θ) yaw</li>
</ul>
</li>
</ul>
<h5 id="projection">Projection</h5>
<p><img src="https://scepter914.github.io/uploads/r/mvlidarnet_2.png" alt=""></p>
<ul>
<li>以下考察</li>
<li>BEV semantic image のinput: 生dataのlidar featureを入力する
<ul>
<li>&quot; Using class probabilities (rather than the most likely class) enables the network to perform</li>
</ul>
</li>
</ul>
<p>ore complex reasoning about the data (e.g., a person on a bicycle); we experimented with both and found this approach to yield better results&quot;</p>
<ul>
<li>
<ul>
<li>probabilities = scoreみたいな話だろうか
<ul>
<li>複数点入っていたらscoreの平均だろうか？</li>
</ul>
</li>
</ul>
</li>
<li>
<p>処理予想</p>
<ul>
<li>Lidar range image (10, 64, 2048) 点の有無、depth, intensity, 7class scores</li>
<li>for point
<ul>
<li>(u, v)座標系 -&gt; x, y, z計算</li>
<li>BEV point_num += 1</li>
<li>BEV semantic image にscore足す + intensity足す</li>
<li>depthはmin？max? ave?</li>
<li>max height, min heightの更新</li>
</ul>
</li>
<li>intensity / point_num, score / point_num</li>
</ul>
</li>
<li>
<p>できあがっているもの</p>
<ul>
<li>BEV semantic image (7, 1024, 1024)</li>
<li>BEV lidar feature (min height, max height, mean intensity) (3, 1024, 1024)</li>
</ul>
</li>
</ul>
<h5 id="network">network</h5>
<p><img src="https://scepter914.github.io/uploads/r/mvlidarnet_4.png" alt=""></p>
<ul>
<li>Loss function
<ul>
<li>focal loss for the classification head and L1 loss for the regression head</li>
</ul>
</li>
<li>Network
<ul>
<li>それぞれのinputを feature map (32, 512, 512)にしてconcatしている</li>
<li>その後classとbboxで別れる</li>
</ul>
</li>
</ul>
<h5 id="clustering">clustering</h5>
<ul>
<li>DBSCAN algorithm
<ul>
<li>“M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, “A density-based algorithm for discovering clusters in large spatial databases with noise,” in International Conference on Knowledge Discovery and Data Mining (KDD), 1996.”</li>
</ul>
</li>
<li>classのscoreでthreshold超えたものに対して行う</li>
</ul>
<h3 id="experiment">Experiment</h3>
<ul>
<li>3d detectionで何しているかを他の人にぱっと見せるのに非常にわかりやすい図
<ul>
<li>こういう気配り的なものもいい論文だなと思う</li>
</ul>
</li>
</ul>
<p><img src="https://scepter914.github.io/uploads/r/mvlidarnet_5.png" alt=""></p>
<ul>
<li>2 stageは一気通貫でもtrainingできるが、データ・セットが対応していないので別々にtrainしている
<ul>
<li>segmentation dataset: semantic kitti</li>
<li>object detection dataset: kitti</li>
</ul>
</li>
<li>semantic KITTIとオレオレデータセットの両方で結果を示した</li>
<li>param
<ul>
<li>BEV size: 80m *80m</li>
<li>cell size(1024): 7.8cm *7.8cm -&gt; output(256) 31.3 cm</li>
</ul>
</li>
<li>TensorRTを使用</li>
</ul>
<h3 id="discussion">Discussion</h3>
<ul>
<li>非常に理にかなったframework
<ul>
<li>range image + BEVなら2dで考えられるのでデバッグが非常にしやすい</li>
<li>range image basedやBEV baseを</li>
</ul>
</li>
<li>2-stageの中では拡張性が高いので使いやすいと思われる</li>
</ul>

</article>
</div>


                
                    
                
            </div>
<footer>
    <article>&copy; scepter914 2019</article>
</footer>

</main>
    </body>
</html>
